{
    "contents" : "config <- function( db='gscholar', cl='ucsd', ms='prob', pr='ins')\n{\n  #db data base to use\n  #cl classifciation\n  #ms measure to find the proximity\n  #pr producer level, i.e. institution ins, individual ind, country cnt\n  library(igraph)\n  #library(ape)\n  dataset <<- toupper(db)\n  taxo <<- toupper(cl)\n  granu <<- toupper(pr)\n  path_data <<- \"~/Dropbox/doctorado/MIT_PROJECT/TESIS_RESEARCH_SPACE/DATA\"\n  path_data_set <<-file.path(path_data, paste(dataset,taxo,granu, sep = '_') )\n  path_taxonomy <<- file.path(path_data, 'TAXONOMIES', taxo)\n  path_producers <<- file.path(path_data, 'LINKS_PRODUCERS')\n  producers <<- get_producers()\n  path_raw <<- file.path(path_data_set, 'RAW')\n  path_benchmark <<- file.path(path_data, 'BENCHMARK')\n  path_overlay <<- file.path(path_data_set, 'OVERLAYS')\n  path_rs <<- file.path(path_data, 'RESEARCH_SPACE', taxo )\n  nodes <<- read.csv(file.path(path_taxonomy,'nodes.csv')) #we assume that a file nodes.csv exists in each taxonomy folder\n  links <<- read.csv(file.path(path_taxonomy, 'edges.csv')) #we assume that there is a file edges.csv in each taxonomy folder\n  setup_benchmarks()\n  setup_datasets()\n}\n\nget_producers <- function()\n{\n  if(granu=='INS')\n  {\n    prod <- read.csv(file.path(path_producers, 'institutions.csv'))\n    colnames(prod)[2] <- 'name'\n  }\n  if(granu=='CNT')\n  {\n    prod <- read.csv(file.path(path_producers, 'countries.csv'))\n    prod <- prod[,c(3,2,1)]\n    colnames(prod) <- c('id','name','id_opus')\n  }\n  if(granu=='IND')\n  {\n    prod <- read.csv(file.path(path_producers,'authors.csv'), colClasses=c(\"factor\", NA, 'NULL', 'NULL', 'NULL',NA,'NULL','NULL','NULL') )\n    colnames(prod) <- c('id', 'name', 'ins_domain')\n  }\n  return(prod)  \n}\n\nsetup_benchmarks <- function()\n{\n  if(taxo == \"UCSD\")\n  {\n    bench_credit <<- \"Börner, K. et al. Design and Update of a Classification System: The UCSD Map of Science. PLoS ONE 7, e39464 (2012).\"\n    bench_source <<- \"ISI and SCOPUS\"\n    bench_interval <<- \"2001-2010\"\n    bench_color <<- \"13 Areas of Science (original colors)\"\n    bench_size <<- \"Production of Science\"\n    bench_unit <<- \"Journals\"\n    bench_tech <<- \"Bibliographic Coupling\" #K50\n  }\n  if(taxo == \"SOM\")\n  {\n    bench_credit <<- \"Rafols, I., Porter, A. L. & Leydesdorff, L. Science overlay maps: A new tool for research policy and library management. Journal of the American Society for Information Science and Technology 61, 1871–1887 (2010).\"\n    bench_source <<- \"ISI\"\n    bench_interval <<- \"2007\"\n    bench_color <<- \"18 Areas of Science  (original colors)\"\n    bench_size <<- \"Production of Science\"\n    bench_unit <<- \"Journals\"\n    bench_tech <<- \"Inter Citation\" #cosine similarity\n  }\n  if(taxo == \"KSA\")\n  {\n    bench_credit <<- \"King of South Arabia Report (2013).\"\n    bench_source <<- \"Google Scholar with SCimago Categories\"\n    bench_interval <<- \"2000-2012\"\n    bench_color <<- \"27 Areas of Science  (original colors)\"\n    bench_size <<- \"Production of Science\"\n    bench_unit <<- \"Researchers\"\n    bench_tech <<- \"Conditional Probability\" #cosine similarity\n  }\n  if(taxo == \"RSPACE-RCA\")\n  {\n    bench_credit <<- \"Research Space Experiments 2015.\"\n    bench_source <<- \"Google Scholar with UCSD Categories\"\n    bench_interval <<- \"2000-2010\"\n    bench_color <<- \"13 Areas of Science  (original colors)\"\n    bench_size <<- \"Production of Science\"\n    bench_unit <<- \"Researchers\"\n    bench_tech <<- \"RCA with cosine similarity\" #cosine similarity\n  }\n  \n  if(taxo == \"RSPACE-TFIDF\")\n  {\n    bench_credit <<- \"Research Space Experiments 2015.\"\n    bench_source <<- \"Google Scholar with UCSD Categories\"\n    bench_interval <<- \"2000-2010\"\n    bench_color <<- \"13 Areas of Science  (original colors)\"\n    bench_size <<- \"Production of Science\"\n    bench_unit <<- \"Researchers\"\n    bench_tech <<- \"TfIdf with cosine similarity\" #cosine similarity\n  }\n  \n}\n\nsetup_datasets <- function()\n{\n  if(dataset == 'GSCHOLAR')\n  {\n    col_classes <<- c(NA, 'factor', 'factor',NA,NA,NA,NA)\n    data_abr <<- 'GS'\n  }\n\t\n\tif(dataset == 'SCIMAGO')\n\t{\n\t\tcol_classes <<- c( NA, 'factor','factor', NA,NA) #year MUST NOT be a factor but an integer\n\t\tdata_abr <<- 'SCIMAGO'\n\t}\n}\n\ndefine_paths_interval <- function()\n{\n  path_interval <<- file.path(path_data_set, 'ANALYSIS', interval_label)\n    dir.create(path_interval, showWarnings=FALSE)\n  gephi_dir <<- file.path(path_interval, 'GEPHI')\n    dir.create(gephi_dir, showWarnings=FALSE)\n  export_dir <<- file.path(path_interval, 'PROCESSED')\n    dir.create(export_dir, showWarnings=FALSE)\n  graphs_dir <<- file.path(path_interval,'GRAPHS')\n    dir.create(graphs_dir, showWarnings=FALSE)\n  images_dir <<- file.path(path_interval, 'IMAGES')\n    dir.create(images_dir, showWarnings=FALSE)\n}\n\n\n\nload_data <- function(n=-1){\n  #print(\"Loading RAW DATA\")\n  if(!exists(\"dataset\")) config()\n  \n  data <- data.frame()\n  files <- list.files(path_raw)\n  \n  for(file in files){\n    #finding a proper separator\n    sep<- c(';',',','\\t')\n    i <- 1\n    mat <- data.frame()\n    while(length(mat) <= 1) #detecting separator automaticaly \n    {\n      print(paste(\"Trying to read file\", file, \"with separator\", sep[i]))\n      mat <- read.csv(file.path(path_raw,file), nrows=3, sep=sep[i], colClasses=col_classes)\n      if(length(mat)>1)\n        sepr <- sep[i] #separator found\n      i <- i + 1\n    } \n    \n    data <- rbind(data, read.csv(file.path(path_raw,file), nrows=n, sep=sepr, colClasses=col_classes))\n    #data <<- read.csv(file.path(path_raw,file), nrows=10, sep=';', header = TRUE)\n  }\n  #print(summary(data))\n  #changing name\n  colnames(data)[2] <- 'id_author' #assuming second column has the proper id producer\n  return(data)\n}\n\nload_data_interval <- function(init, end, feat='wgh_jfrac', agg='mean')\n{\n  #print(\"Loading RAW DATA...\")\n  if(!exists(\"data_raw\"))   data_raw <<- load_data() \n  \n  #print(\"Loading Interval DATA...\")\n  data <- subset(data_raw, year>= init & year<= end)\n  #print(summary(data))\n  \n  #global variables for the names and information of the interval\n  file_name <<- paste(data_abr, taxo, feat, init, end, agg, sep=\"_\")\n  n_registers <<- length(data$year)\n  n_years <<- length(unique(data$year))\n  n_authors <<- length(unique(data$id_author))\n  n_authorships <<- sum(data$authorship_count)\n  n_fields <<- length(unique(data$subdiscipline_id))\n  \n  #aggregating \n  if(agg=='sum') fun_agg<- sum\n  if(agg=='mean') fun_agg<- mean\n  if(agg=='median') fun_agg<-median\n  if(!is.na(agg))\n  {\n    data <- aggregate(x=data[feat], by=list(id_author=data$id_author, id_category = data$subdiscipline_id), FUN=fun_agg)  \n  }\n  \n  \n  #print(summary(data))\n  \n  return(data)\n}\n  \n#MAIN FUNCTION\nget_network <- function(init=-1, end=-1, by=-1, n=-1)  #main function\n{\n  data_raw <<- load_data(n)  \n  get_intervals(year_ini=init, year_fin=end, win=by)\n  \n  for(interval in intervals)\n  {\n    i <- match(interval, intervals) #index of parameter interval\n    data <- load_data_interval(init=year_start[i], end=year_end[i])\n    net <- cond_prob(data) \n    geph <- export_gephi(net)  \n  }\n  return(net)\n}\n\nget_intervals <- function(year_ini=-1, year_fin=-1, win=-1, decade=FALSE)\n{\n  years <- unique(data_raw$year)\n  min_year <- min(years)\n  max_year <- max(years)\n  #subseting interval\n  if(year_ini == -1 ){ year_ini <- min_year}\n  if(year_fin == -1 ) {year_fin <- max_year }\n  if(win==-1){ win <- year_fin - year_ini + 1}  #take the whole interval\n\n  year_start <<- vector()\n  year_end <<- vector()\n  intervals <<- vector()\n  \n  if(year_ini < min_year)\n  {\n    print(paste(\"Initial year Must be greater than\", min_year))\n    stop(\"Finalizando\")\n  } else\n  {\n    n_intervals <<- floor(((year_fin -year_ini)+1)/win)\n    y_s <- year_ini\n    for(i in 1:n_intervals)\n    {\n      year_start[i] <<- y_s\n      year_end[i] <<- y_s + (win - 1)\n      intervals[i] <<- paste(year_start[i],year_end[i],sep=\"_\")\n      y_e <- year_end[i]\n      y_s <- y_s + win\n    }  \n    interval_label <<- paste(\"pan\",year_ini,y_e,\"by\",win, sep=\"_\")\n    define_paths_interval()\n  }\n  \n  #print(paste(\"year_start\", year_start))\n  #print(paste(\"year_end\", year_end))\n}\n\n\nget_intervals_overlay <- function(year_ini=-1, year_fin=-1, win=-1, decade=FALSE)\n{\n  years <- unique(data_raw$year)\n  min_year <- min(years)\n  max_year <- max(years)\n  #subseting interval\n  if(year_ini == -1 ){ year_ini <- min_year}\n  if(year_fin == -1 ) {year_fin <- max_year }\n  if(win==-1){ win <- year_fin - year_ini + 1}  #take the whole interval\n  \n  year_start <<- vector()\n  year_end <<- vector()\n  intervals <<- vector()\n  \n  if(year_ini < min_year)\n  {\n    print(paste(\"Initial year Must be greater than\", min_year))\n    stop(\"Finalizando\")\n  } else\n  {\n    n_intervals <<- floor(((year_fin -year_ini)+1)/win)\n    y_s <- year_ini\n    for(i in 1:n_intervals)\n    {\n      year_start[i] <<- y_s\n      year_end[i] <<- y_s + (win - 1)\n      intervals[i] <<- paste(year_start[i],year_end[i],sep=\"_\")\n      y_e <- year_end[i]\n      y_s <- y_s + win\n    }  \n    interval_label <<- paste(\"pan\",year_ini,y_e,\"by\",win, sep=\"_\")\n    path_interval_overlay <<- file.path(path_overlay, interval_label)\n    dir.create(path_interval_overlay, showWarnings=FALSE)\n  }\n  \n  #print(paste(\"year_start\", year_start))\n  #print(paste(\"year_end\", year_end))\n}\n\n\n\n\ncond_prob <- function(data)\n{\n  # data must has this shape  producer | category | value\n  library(\"reshape2\")\n  \n  M_raw <- acast(data, id_author~id_category, fun.aggregate = sum) #use count for binary matrix\n  categ <- colnames(M_raw)\n  n_ctg <- length(categ)\n  \n  \n  freq <- matrix(0.0, nrow=n_ctg, ncol=n_ctg) #creating empty matrix\n  colnames(freq) <- categ\n  row.names(freq) <- categ\n  \n  \n  prob_cond <- freq\n  prob_join <- freq\n  phi <- freq # matrix of distances using Max conditional probability according to P. Space  \n  \n  M <- M_raw / rowSums(M_raw)  #shares or values, this means a normalization by the total production of the author\n  k <- colSums(M) #sum of values over the categories\n  \n  freq <- t(as.matrix(M)) %*% as.matrix(M)\n  prob_join <- freq/n_ctg  #simetric matrix Not used but maybe useful in the future\n  prob_conditional <- freq / k  #k stores the total , NOT simetric matrix\n  \n  write.csv(prob_conditional, file.path(export_dir,paste(file_name,'_cond_prob_both.csv', sep=\"\")))\n  #defining the matrix of distances or proximities with the Min case, this is the Max Ubiquity in the denominator\n  #phi is the matrix of distances\n  for(i in 1:n_ctg){ #product p\n    for(j in 1:n_ctg)\n    {\n      phi[i,j]<-phi[j,i]<-min(prob_conditional[i,j],prob_conditional[j,i])\n    }\n  }\n  write.csv(prob_conditional, file.path(export_dir,paste(file_name,'_cond_prob_min.csv',sep=\"\")))\n  \n  #computing MST\n  #preparing export of edges\n  \n  \n  get_mst(phi_for_mst)\n  \n  phi_ret <- melt(phi, na.rm = TRUE)  #EDGES LIST or PANEL SHAPE\n  #return(phi_ret)\n  return(prob_conditional)\n}\n\nget_graph <- function(phi,file_n=file_name, type='MST') #TYPE indicates which type of graph, as MST | TTest | Both\n{\n  #phi is a matrix equitriangular with proximities or similarities\n  phi[phi==0] <- NA\n  phi[upper.tri(phi, diag=TRUE)] <- NA\n  phi_graph <- phi\n  phi_graph[is.na(phi_graph)] <- 0\n  phi_graph <- as.matrix(phi_graph)\n  \n  if(type=='MST')\n  {\n    #Create MinumumSpanningTree\n    #d <<- dist(phi) # we are using euclidean distance for default\n    d <<- 1-phi_graph  #In order to calculate MST we need a matrix of DISTANCES, this is 1 minus proximity phi.\n    phi_mst_binary <- mst(d) #matrix with binary values of mst\n    phi_graph <- as.matrix(phi_mst_binary)  #matrix with real values of mst \n  }\n  \n  \n  \n  gms <<- graph.adjacency(adjmatrix = phi_graph,mode = 'undirected',weighted = TRUE)\n  \n  if(type=='TTest')\n  {\n    g_temp <- decompose.graph(gms)\n    gms <<- g_temp[[1]] #biggest connected component\n  }\n  \n  #lower aggregation\n  V(gms)$Label = as.character(nodes$subd_name[match(V(gms)$name, nodes$Id)]) #watch out with the name of the column!!\n  #higher aggregation\n  V(gms)$Field = as.character(nodes$subd_name[match(V(gms)$name, nodes$Id)]) #watch out with the name of the column!!\n  V(gms)$Area = as.character(nodes$Discipline[match(V(gms)$name, nodes$Id)]) #watch out with the name of the column!!\n  #adding color to vertex\n  V(gms)$label = V(gms)$Field\n  V(gms)$size = (degree(gms)/max(degree(gms))) * 6\n  \n  V(gms)$color = as.character(nodes$color[match(V(gms)$name, nodes$Id)])\n  #gms$layout <- layout.drl(gms)\n  #plotMST(gms,'drl')\n  #gms$layout <- layout.auto(gms)\n  #plotMST(gms,'auto')\n  #gms$layout <- layout.circle(gms)\n  #plotMST(gms,'circle')\n  gms$layout <- layout.fruchterman.reingold(gms)\n  plot_graph(gms,lay='fruchterman.reingold',title=paste(file_n,type,sep='_'))\n  \n  write.graph(gms,file=file.path(graphs_dir, paste(file_n,'_',type,'.gml', sep='')), format='gml')\n  return(phi_graph)\n}\n\n#used to plot created maps (by us)\nplot_graph <- function(g,lay='',title=paste('UNtype',file_name))\n{\n  set.seed(77)\n  #plot.igraph(g, vertex.size=5, vertex.label.cex=0.5, asp=FALSE,main=title)\n # plot.igraph(g,  vertex.label.cex=0.7, edge.curved=TRUE,  vertex.label.font=0, vertex.label.family='Arial', vertex.label.color='black', asp=FALSE,main=title)\n  #plot.igraph(g, sub=paste('Layout:' ,subt, '. Size: degree.', 'Colored by Area of Science.'), vertex.label.cex=0.7, vertex.label.font=0, vertex.label.family='Arial', vertex.label.color='black',main=title, asp=FALSE)\n info_template <- paste('| Layout:' ,lay, '| Size: degree.', '| Colored by Area of Science.')\n info_interval <- paste('Registers:' , n_registers, '| Authors:', n_authors,'| Authorships:', n_authorships, '| Fields:', n_fields, '| Years:', n_years)\n \n plot.igraph(g, sub=paste(info_interval, info_template), vertex.label.cex=0.7, vertex.label.font=0, vertex.label.family='Helvetica', vertex.label.color='black',main=title, asp=FALSE)\n #dev.off()\n dev_file_name <- file.path(images_dir,paste(title,'.pdf', sep='_'))\n #dev.print(pdf, file=dev_file_name, widht=6, height=3 );\n pdf(dev_file_name, width=16, height=12, family='Helvetica', pointsize=8)\n   \n   \n  plot.igraph(g, sub=paste(info_interval, info_template), vertex.label.cex=0.7, vertex.label.font=0, vertex.label.family='Helvetica',  vertex.label.color='black',main=title, asp=FALSE)\n dev.off()\n  #dev.copy(pdf,filename=dev_file_name, family='Helvetica');\n  #dev.off ();\n \n}\n\n#used to plot benchmark Maps of Science\nplot_graph_base <- function(g,layout='fr',title=paste('Benchmark Map -',taxo), size=bench_size, lab=FALSE, cex=1)\n{\n  set.seed(77)\n  #gms$layout <- layout.drl(gms)\n  #plotMST(gms,'drl')\n  #gms$layout <- layout.auto(gms)\n  #plotMST(gms,'auto')\n  #gms$layout <- layout.circle(gms)\n  #plotMST(gms,'circle')\n  if(layout=='fr')\n  {\n    g$layout <- layout.fruchterman.reingold(g) \n    lay <- 'Fruchterman Reingold'\n  }\n  \n  V(g)$Label = as.character(nodes$subd_name[match(V(g)$name, nodes$Id)]) #watch out with the name of the column!!\n  #higher aggregation\n  V(g)$Field = as.character(nodes$subd_name[match(V(g)$name, nodes$Id)]) #watch out with the name of the column!!\n  V(g)$Area = as.character(nodes$Discipline[match(V(g)$name, nodes$Id)]) #watch out with the name of the column!!\n  #adding LABELS IF REQUIRED\n  V(g)$label<-''\n  if(lab==TRUE)\n  {\n    V(g)$label = V(g)$Field\n  }\n  # V(gms)$size = (degree(gms)/max(degree(gms))) * 6\n  if(size=='degree')\n  {\n    V(g)$size = (degree(g)/max(degree(g))) * 4  \n  }\n  \n  V(g)$color = as.character(nodes$color[match(V(g)$name, nodes$Id)])  \n  #plot.igraph(g, vertex.size=5, vertex.label.cex=0.5, asp=FALSE,main=title)\n  # plot.igraph(g,  vertex.label.cex=0.7, edge.curved=TRUE,  vertex.label.font=0, vertex.label.family='Arial', vertex.label.color='black', asp=FALSE,main=title)\n  #plot.igraph(g, sub=paste('Layout:' ,subt, '. Size: degree.', 'Colored by Area of Science.'), vertex.label.cex=0.7, vertex.label.font=0, vertex.label.family='Arial', vertex.label.color='black',main=title, asp=FALSE)\n  info_credit <- bench_credit\n  info_template <- paste('Layout WE applied:' , lay, '| Size:', size, '| Colored by:', bench_color)\n  info_interval <- paste('Data time: ', bench_interval, ' | Data Source: ', bench_source, ' | Unit of analysis: ', bench_unit, '| Technnique: ', bench_tech)\n  \n  plot.igraph(g, \n  \tsub=list(paste(info_credit,  info_interval, info_template, sep='\\n'), cex=0.8*cex), \n  \tvertex.label.cex=0.6*cex, \n  \tvertex.label.font=0, \n  \tvertex.label.family='Helvetica', \n  \tvertex.label.color='black',\n  \tmain=list(title,cex=1*0.8), \n  \tasp=FALSE)\n  #dev.off()\n  dev_file_name <- file.path(path_benchmark,paste(title,'.pdf', sep=''))\n  #dev.print(pdf, file=dev_file_name, widht=6, height=3 );\n  pdf(dev_file_name, width=16, height=12, family='Helvetica', pointsize=8)\n  \n  \n  plot.igraph(g, \n  \tsub=list(paste(info_credit, info_interval,  info_template,  sep='\\n'), cex=0.8*cex), \n  \tvertex.label.cex=0.7, vertex.label.font=0, vertex.label.family='Helvetica',  \n  \tvertex.label.color='black',\n  \tmain=list(title,cex=1*cex),\n  \tasp=FALSE)\n  dev.off()\n  #dev.copy(pdf,filename=dev_file_name, family='Helvetica');\n  #dev.off ();\n  \n}\n\n\n\n\n\nexport_gephi <- function(phi)\n{\n  Source<-phi[,1]\n  Target<-phi[,2]\n  Weight <- phi[,3]\n  Label <- phi[,3]\n  #Type <- character()\n  #MST <- character(0) \n  #Filter <- character()\n  #filter <- 0.56621325 # average 4 for scimago 2012\n  filter <- 0.2780202 # average 4 for gscholar 2000-2013 \n  matr_list <- data.frame(Source,Target,Label,Weight)\n  f_name <- paste(file_name,'_EDGES.csv',sep=\"\")\n  write.csv(matr_list, file.path(gephi_dir,f_name), row.names=FALSE)\n  \n  return(matr_list)\n}\n\nexport_igraph <- function(phi)\n{\n  library(\"igraph\")\n}\n\n\nget_benchmark_map<- function(layout='fr', rs=NULL, pl=FALSE, lab=FALSE, pl_mst=FALSE, mean_degree=4, cex=1, mst=FALSE, pl_seed=pl_seed)\n{\n\t\n\t\n\t\tbench_map <- graph.data.frame(links, directed = TRUE)\n\t\t#bench_map <- as.undirected(bench_map)\n\t\tprint(list.edge.attributes(bench_map))\n\t\t\n\tif(is.null(rs))\n\t{\n\t\t\t#create benchmark maps\n\t\tgms <- bench_map\n\t\tE(gms)$weight <- E(gms)$weight #just to avoid problems in density full calculation\n\t\t\n\t\tif(pl==TRUE)\n\t\t{\n\t\t\tpar(mfrow=c(1,1))\n\t\t\tplot_graph_base(gms,size='degree', layout=layout, lab=lab)  \n\t\t}\n\t}\n\telse\n\t{\n\t\tpath_rs <- file.path(path_rs, paste(rs,\".RData\",sep=\"\"))\n\t\tgms <- plot_rs(path_rs=path_rs, mean_degree=mean_degree, pl_mst=pl_mst, mst=mst, pl_seed=pl_seed, prop_to_lab=0.3, cex=cex, pl=pl)\n\t\tgms <- remove.edge.attribute(gms, 'weight_1')\n\t\tgms <- remove.edge.attribute(gms, 'weight_2')\n\t\t#E(gms)$weight_bench[E(bench_map)] <- E(bench_map)$weight[E(bench_map)] \n\t\t#print(list.edge.attributes(gms))\n\t\tgms <- graph.union(gms, bench_map)\n\t\tE(gms)$weight <- E(gms)$weight_1\n\t\tE(gms)$weight_bench <- E(gms)$weight_2 #just to track which are the links used by bench map\n\t\tgms <- delete.edges(gms, E(gms)[is.na(E(gms)$weight)])\n\t\t\n\t\tgms <- remove.edge.attribute(gms, 'weight_1')\n\t\tgms <- remove.edge.attribute(gms, 'weight_2')\n\t}\n\t\n\t\n\t\tprint(list.edge.attributes(gms))\n\t\n\treturn(gms)\n\t\n}\n",
    "created" : 1447185345056.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2459184462",
    "id" : "2CF22E2",
    "lastKnownWriteTime" : 1454176383,
    "path" : "~/Dropbox/doctorado/MIT_PROJECT/TESIS_RESEARCH_SPACE/CODES/RS2015/config.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 0,
    "source_on_save" : false,
    "type" : "r_source"
}