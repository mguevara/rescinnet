{
    "contents" : "#generate a network of similarities between areas of science.\n#source config.R first!\nlibrary(diverse)\n\n#aggregate and filtrate raw data to get an aggregated data to build a research space\nget_data_rs <- function(n=600, init=2001, end=2012, pl=FALSE, min_prod=0, min_awgh=0, min_jfrac=0, min_awjf=0, feat='authorship_count', agg_fun=sum, list_prod=NA)\n{\n    #get sample of producers\n    print(paste(\"Subseting data in interval: \", init, \"_\", end, sep=''))\n    data_interval <- load_data_interval(init = init, end = end, agg=NaN) #without aggregation #change here the sep!!!\n    num_years <- end-init\n    #ensuring a minimum of publications min_prod per number of years in the interval of time\n    #data_interval <<- subset(data_interval, authorship_count > (num_years * min_prod)) #subset according number of authorships\n    #calculating totals \n    print(paste(\"Total authors in interval\",  \":\", length(unique(data_interval$id_author)), sep=''))\n    #unique(data_interval$id_author)\n    data_sample <- data_interval\n    ##filtering\n    if(min_prod != 0)  #by average production in total interval \n    {\n        print(\"Filtering by total production...\")\n\t      totals_producers <- aggregate(authorship_count~id_author , data_interval, FUN=sum)  \n\t      #print(head(totals_producers))\n\t      #str(totals_producers)\n\t      #totals_producers <- aggregate(wgh_jfrac~id_author, data_interval, FUN=sum)  \n\t      #producers accomplish min production\n\t      #producers <- totals_producers$id_author[totals_producers$authorship_count> (num_years * min_prod)]\n\t      producers_ids <- totals_producers$id_author[totals_producers[, feat]> (num_years * min_prod)]\n\t      data_sample <- subset(data_interval, id_author %in% producers_ids)  #watch out with the name of the column of producers.\n\t    \n    }\n    producers_ids <- unique(data_sample$id_author) #\n    #filtering per features of authors\n    print(paste(\"Authors:\", length(producers_ids)))\n    #filtering per weighted authorship\n    if(min_awgh != 0) #by author weighted times journal fractional assigment \n    {\n      print(\"Filtering by weighted authorship...\")\n      data_sample <- subset(data_sample, authorship_wgh >= min_awgh)  #watch out with the name of the column of\n      producers_ids <- unique(data_sample$id_author)\n      print(paste(\"Authors:\", length(producers_ids)))\n    }\n    if(min_jfrac != 0) #by author weighted times journal fractional assigment \n    {\n      print(\"Filtering by journal fractional assignment to categories...\")\n      data_sample <- subset(data_sample, count_jfrac >= min_jfrac)  #watch out with the name of the column of\n      producers_ids <- unique(data_sample$id_author) #\n      print(paste(\"Authors:\", length(producers_ids)))\n    }\n    if(min_awjf != 0) #by author weighted times journal fractional assigment \n    {\n      print(\"Filtering by weighted authorship times journal fractional assignment...\")\n      data_sample <- subset(data_sample, wgh_jfrac >= min_awjf)  #watch out with the name of the column of\n      producers_ids <- unique(data_sample$id_author) #\n      print(paste(\"Authors:\", length(producers_ids)))\n    }\n    #choosing a subset according to parameter n\n    if(n!=-1)\n    {\n      if(n > length(producers_ids))\n        stop(\"The sample required N, is less than the total number of producers\")\n      print(paste(\"Choosing a sample of\", n, \"authors...\"))\n      producers_sample <- producers_ids[sample(length(producers_ids), n,replace = FALSE)]\n      data_sample <- subset(data_sample, id_author %in% producers_sample)  #watch out with the name of the column of producers.\n      producers_ids <- unique(data_sample$id_author) #\n      print(paste(\"Authors:\", length(producers_ids)))\n    }\n    rs_n_auth <<- length(producers_ids)\n    rs_n_authship <<- sum(data_sample$authorship_count)\n    print(paste(\"Total authorships:\", rs_n_authship))\n    \n    print(\"Cleanning NA values\")\n    print( data_sample[is.na(data_sample[,feat]),])\n    data_sample <- data_sample[!is.na(data_sample[,feat]),]\n    \n    print(\"Testing NA values\")\n    print( data_sample[is.na(data_sample[,feat]),])\n    \n    print(paste(\"Aggregating data per\", feat,\"...\"))\n    data_agg <-  aggregate(x=data_sample[feat], by=list(id_author = data_sample$id_author, id_category = data_sample$subdiscipline_id), FUN=agg_fun) #total of that category\n    #data_agg<- load_data_interval(init=init, end=end, feat='authorship_count', agg=agg_fun)\n    data_to_use_for_rs <<- data_agg\n    producers_used_for_rs <<- producers_ids\n    \n  return(data_agg)\n}\n\n# n number of authors to use after filter\n# init initial year to make the aggregation\n# end final year to make the aggregation\n# pl boolean to indicate wheter to plot or not the rspace\n# min_prod minimum production for each year\n# min_awgh minimum weighted coauthorship (i.e. 0.5 2 total authors in the paper)\n# min_jfrac minimum fractional assigment of the journal to the category\n# min_awjf minimum weighted coauthorship and fractional journal assignement\n\n\n#creates a research space\ncreate_rs <- function(n=100, init=2012, end=2014, pl=FALSE,  min_prod=0, min_awgh=0, min_jfrac=0, min_awjf=0, feat='authorship_count', agg_fun=sum,  sim='n', mean_degree=4, mst=TRUE, pl_seed=3, dir_otn='')\n{\n  library(diverse)\n  library(pheatmap)\n  data_authorship <- get_data_rs(n=n, init=init, end=end, min_prod=min_prod, min_awgh=min_awgh, min_jfrac=min_jfrac, min_awjf=min_awjf,feat=feat, agg_fun=agg_fun) #returns an edge list\n  #cleaning factors or perish!\n  data_authorship$id_author <- factor(data_authorship$id_author)\n  data_authorship$id_category <- factor(data_authorship$id_category)\n  #cleaning NAs\n  \n  #not so eficient\n    #data_mat <- values(data_authorship)\n    #mat_dis <- dis_categories(data=data_authorship, method ='cosine' )\n  print(\"Creating matrix Authors-Categories...\")  \n  mat_values <- values(data_authorship) #depends on package DIVERSE\n  print(\"Binarizing matrix...\")\n  mat_values[mat_values>0] <- 1  #BINARIZATION NEEDED! we want to know if a user publishes in some field, just that.\n  #dot product to find the number of joint authorships\n  print(\"Computing number of authors in joint areas...\")\n  mat_dis_values <- t(mat_values) %*% (mat_values) \n  \n  \n  #mat_dis_values_deb <<- mat_dis_values\n  rs_cat_not_connected <- rownames(mat_dis_values)[rowSums(mat_dis_values) == 0]\n  \n  print(paste(\"Number of categories not conected in CALCULATIONS\", length(rs_cat_not_connected)))\n  \n  cat_connected <- rownames(mat_dis_values)[rowSums(mat_dis_values) != 0]\n  \n  #cleanning un connected categories\n  mat_dis_values <- mat_dis_values[cat_connected, cat_connected]\n  \n  rs_inner_values <- mat_dis_values #to store joint occurrence\n  \n  if(sim=='pr')\n  {\n    tot_cat <- diag(mat_dis_values)\n    print(\"total cat\")\n    print(tot_cat)\n    mat_tot_cat <- matrix(tot_cat, nrow = length(tot_cat), ncol = length(tot_cat), byrow = TRUE)#the diagonal contains the totals per category\n    #print(\"Mat total cat\")\n    #print(mat_tot_cat)\n    mat_dis_values <- mat_dis_values / mat_tot_cat  #computing the conditional probability in each cell\n    print(\"Mat with prob\")\n    print(mat_dis_values)\n  }\n  adj <- mat_dis_values\n  \n  #pheatmap(adj, cluster_rows=FALSE, cluster_cols=FALSE)\n  #hist(adj)\n  #adj[adj < 210] <- 0  #cosine 0.06 adj[adj > 0.06\n  #hist(adj)\n  mat_final <<- adj\n  mat_values <<- mat_dis_values\n  print(\"Writing matrices to external files...\")\n  file_name <- paste('New_rs_sim',sim, init, end, 'n',n , 'aw',min_awgh, 'jf', min_jfrac, 'awjf', min_awjf, 'min_prod', min_prod, sep='_'  )\n  #write.csv(mat_final, file=file.path(path_rs, paste(file_name,\".csv\",sep='')))\n\t#print(paste(\"matrix of similarities in: \", file.path(path_rs, file_name)))\n\t\n\trs_adj <- adj\n\trs_taxo <- taxo\n\trs_data <- dataset\n\trs_init <- init\n\trs_end <- end\n\trs_n <- n\n\trs_sim <- sim\n\trs_min_prod <- min_prod\n\trs_min_awgh <- min_awgh\n\trs_min_jfrac <- min_jfrac\n\trs_min_awjf <- min_awjf\n\n\t\n\tfile_rdata <- file.path(path_rs, dir_otn,paste(file_name,\".RData\",sep=\"\"))\n\tsave(rs_adj, rs_inner_values, rs_data,  rs_taxo, rs_init, rs_end, rs_n, rs_sim, rs_min_prod, rs_min_awgh, rs_min_jfrac, rs_min_awjf, rs_n_auth, rs_n_authship, rs_cat_not_connected, file=file_rdata)\n\tprint(paste(\"Data of RS created in: \", file_rdata))\n\t\n  if(pl==TRUE)\n  {\n    print(\"Plotting RS...\")\n    plot_rs(path_rs = file_rdata, mean_degree = mean_degree, mst = mst, pl_seed = pl_seed)\n  }\n\t\n\treturn(adj)\n}\n\ncreate_rs_otn <- function(mean_degree=4, pl_mst=FALSE)\n{\n  g <- empty_graph() #to accumulate the MSTs of each interval\n  path_rs <- file.path(path_rs,'OTN')\n  files <- list.files(path = path_rs)\n  i <- 1\n  for(file in files)\n  {\n    g_int <- plot_rs(path_rs = file.path(path_rs, file), pl=FALSE, mean_degree = 1, mst = TRUE, pl_mst=pl_mst)\n    \n    E(g_int)$weight_orig <- E(g_int)$weight\n    \n    if(i==1)\n    {\n      g <- g_int\n      E(g)$n_present <- 1\n    }\n    if(i>1)\n    {\n      print(paste(\"IIII:\",i))\n      #print(paste(\"Entering this interaction\", list.edge.attributes(g)) )\n      print(E(g_int)$weight)\n      \n      E(g_int)$active[E(g_int)$weight>0] <- 1  #detecting active links\n      g <- graph.union(g, g_int)\n      \n      #increasing count for edges in MST in the working interval\n      E(g)$n_present[is.na(E(g)$n_present)] <- 0 #some new commers might have arrived!\n      E(g)$n_present[!is.na(E(g)$active)] <- E(g)$n_present[!is.na(E(g)$active)] + 1 #increasing the count of presence\n      g <- remove.edge.attribute(g, \"active\")\n      \n      #print(paste(\"AFTER remove ACTIVE\", list.edge.attributes(g)))\n      #summing the weights of the previous cummulative graph and the actual interval graph\n      E(g)$weight[!is.na(E(g)$weight_1) & !is.na(E(g)$weight_2)] <- E(g)$weight_1[!is.na(E(g)$weight_1) & !is.na(E(g)$weight_2)] + E(g)$weight_2[!is.na(E(g)$weight_1) & !is.na(E(g)$weight_2)]\n      \n      g <- remove.edge.attribute(g, \"weight_1\")\n      g <- remove.edge.attribute(g, \"weight_2\")\n      \n      #print(paste(\"AFTER weight_1, weight_2 removal\", list.edge.attributes(g)))\n    }\n    \n    #print(E(g)$n_present)\n    i <- i +1 \n  }\n  \n  #computing the average of the Weights cummulated in the total graph\n  E(g)$weight <- E(g)$weight/E(g)$n_present\n  fil <- 1\n  while(trunc(mean(degree(g))) > mean_degree)\n  {\n    g <- delete.edges(g, E(g)[E(g)$n_present < fil]) #aprox mean degree - 4\n    isolated_nodes <- V(g)[degree(g)==0]\n    g <- delete.vertices(g, isolated_nodes)\n    fil <- fil + 1\n  }\n  \n  \n  print(\"Isolated nodes:\")\n  print(isolated_nodes)\n  print(\"Ploting OTN Graph:\")\n  \n  file_name <- paste('rs_sim',sim, init, end, 'n',n , 'aw',min_awgh, 'jf', min_jfrac, 'awjf', min_awjf, 'min_prod', min_prod, sep='_'  )\n  \n  file_rdata <- file.path(path_rs,paste(file_name,\".RData\",sep=\"\"))\n  save(rs_adj, rs_data,  rs_taxo, rs_init, rs_end, rs_n, rs_sim, rs_min_prod, rs_min_awgh, rs_min_jfrac, rs_min_awjf, rs_n_auth, rs_n_authship, file=file_rdata)\n  print(paste(\"Data of RS created in: \", file_rdata))\n  \n  plot_graph_smart(g)\n  print(\"Filtered per n_presence=\", fil-1)\n  g_otn <<- g\n  return(g)\n  \n}\n\n# prop_to_lab threshold of number of nodes to show all labels of the nodes\n#this function takes a research space previously created with function create_rs() in order to filter it and plot it\n#rs_sim_pr_1987_2014_n_-1_aw_0_jf_0_awjf_0.75_min_prod_0\n#rs_sim_pr_2000_2009_n_-1_aw_0_jf_0_awjf_0.75_min_prod_0\nplot_rs <- function(path_rs, mean_degree=4, mst=TRUE, pl_seed=1, prop_to_lab=0.2, cex=1, pl=FALSE, pl_mst=FALSE)\n{\n  library(\"igraph\")\n  load(path_rs) #load complete information of the research space wanted all variables are rs_\n  #adj <- dis_categories(pantheon)\n  #pheatmap(adj, cluster_rows=FALSE, cluster_cols=FALSE)\n  adj_orig <- rs_adj\n\t\n#       \tfilters <-'Filters applied per author: '\n#       \tif(rs_min_awgh>0) filters <- paste(filters, \" Yearly weighted authorship > \", rs_min_awgh , \"|\")\n#       \tif(rs_min_jfrac>0) filters <- paste(filters, \" Yearly fractional journal assignment < \", rs_min_jfrac, \"|\")\n#       \tif(rs_min_awjf>0) filters <-paste(filters, \" Yearly weighted authorship and fractional journal assignment < \", rs_min_awjf, \"|\")\n#       \tif(rs_min_prod>0) filters <- paste(filters, \" Yearly minimum average production:\", rs_min_prod)\n#       \t\n#       \tif(rs_sim=='n') sim_used <- \"Number of authors producing in both areas\"\n#       \tif(rs_sim=='pr') sim_used <- \"Conditional probability of an author publishing in both areas\"\n#       \n#       \ttitle <- paste('Research Space\\n','Data: ', rs_data, '| Taxonomy:' , rs_taxo, '| Interval:' ,rs_init, '-', rs_end) \n#       \tsubtitle <- paste(\"Size of nodes: degree | Colors of nodes: automatic community detection | \", \"Layout type force-directed | \", \" Seed plot \", pl_seed,\n#       \t\t\"\\n\",\"Similarity between nodes:\", sim_used, \"| Amount of authors: \", format(rs_n_auth, big.mark = ','), \" |  Amount of authorships: \", format(rs_n_authship, big.mark=\",\"), \n#       \t\t\"\\n\", filters)\n\t\n  #diag(adj_orig) <- 0\n\tadj_clean <- adj_orig\n\tdiag(adj_clean) <-0\n\t#cleanning NA\n\tadj_clean[is.na(adj_clean)] <- 0\n\t\n  cat_not_connected <- rownames(adj_clean)[rowSums(adj_clean) == 0 | colSums(adj_clean) == 0]\n  print(paste(\"Number of categories not conected\", length(cat_not_connected)))\n  \n  cat_connected <- rownames(adj_clean)[rowSums(adj_clean) != 0 & colSums(adj_clean) != 0] #note that in the case of probability the Min of upper triangular and lower triangular is taken, so connection in both upper and lower triangle of the matrix is required!!!\n  \n  #cleanning un connected categories\n  adj_cc <- adj_orig[cat_connected, cat_connected]\n  adj_cc[is.na(adj_cc)] <- 0 #in case NAs\n  #creating full graph for union future process \n  #adj_full <-  (adj_cc-min(adj_cc,na.rm=TRUE))/(max(adj_cc, na.rm=TRUE)-min(adj_cc, na.rm=TRUE)) #normalizing between zero and one\n  adj_full <- adj_cc\n  #g_full <- graph.adjacency(adjmatrix = adj_full, mode = \"min\", weighted = TRUE, diag = FALSE) #note minimum because the min probability ORIGINAL\n  g_full <- graph.adjacency(adjmatrix = adj_full, mode = \"directed\", weighted = TRUE, diag = FALSE) #modified to be directed\n  \n  #HARDCODED\n  #if(mst==TRUE)\n  if(TRUE==TRUE)\n  {\n  \tprint(\"Computing minimum spnanning tree\")\n  \t#normalize similarities\n  \t#dist_matrix <- 1 - (adj_cc-min(adj_cc,na.rm=TRUE))/(max(adj_cc, na.rm=TRUE)-min(adj_cc, na.rm=TRUE)) #normalizing between zero and one\n  \tdist_matrix <- 1 - (adj_cc)/(max(adj_cc, na.rm=TRUE)) #normalizing per le maximum, numerator should not has values of zero!\n  \t#MST must use the DISTANCE matrix NOT the similarities!!!!!\n    #g_mst <-  graph.adjacency(adjmatrix = dist_matrix, mode = \"max\", weighted = TRUE, diag = FALSE) #max is the worst case ORIGINAL\n    g_mst <-  graph.adjacency(adjmatrix = dist_matrix, mode = \"directed\", weighted = TRUE, diag = FALSE) #max is the worst case\n    #g_mst <- simplify(g_mst, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = list(weight=\"min\", \"ignore\")) #taking the minimum of the probability\n    #E(g_mst)$sim <- -1*(E(g_mst)$weight -1)\n    #g_original <<- g_mst\n    #g_mst=delete.edges(g_mst, which(E(g_mst)$weight <=1)) # here's my condition.\n    \n    g_mst <- minimum.spanning.tree(g_mst)\n    \n    if(pl_mst==TRUE)\n    {\n      main <- paste( title, \"\\n Minimum Spanning Tree\")\n      par(mfrow=c(1,2))\n      plot_graph_smart(g_mst, main=main, lay = 'fr', v_label = 'no', v_size='degree', cex = cex) #force directed\n      plot_graph_smart(g_mst, main=main, lay = 'rt', v_label = 'no', v_size='degree', cex = cex) #tree\n      par(mfrow=c(1,1))\n      plot_graph_smart(g_mst, main=main, lay = 'rt', v_label = 'com', v_size=2, cex = cex) #tree\n    }\n  \t\n    \n  \t\t#flagging edges of Minimum ST for future combination\n  \t\tE(g_mst)$mst <- 1\n  } \n\n  #######THERSHOLD up to certain mean degree (ideally 4)\n\n  for(mean_deg in mean_degree )\n  {\n  \t\tprint(\"Computing threshold graph\")\n  \t  filter <- min(rs_adj) #initial filter \n  \t  #if(rs_sim=='n') increment <- 1\n  \t  #if(rs_sim=='pr') increment <- 0.001 \n  \t  \n  \t  #HARDCODED\n  \t  increment <- 1\n  \t\t#end HARDCODED\n  \t  \n  \t  degree_rs <- mean_deg + 1 #to go into the loop\n  \t\tadj_th <- adj_cc\n  \t\t#adj_th <-  (adj_th-min(adj_th,na.rm=TRUE))/(max(adj_th, na.rm=TRUE)-min(adj_th, na.rm=TRUE)) #normalizing between zero and one\n  \t\t\n\t\t  #while(degree_rs > mean_deg)#HARDCODED\n  \t\tkk <- 1\n  \t\tkkk <- 0\n  \t\twhile(kk > kkk )\n\t\t  {\n\t\t  \t#print(\"here!!\")\n\t\t    adj_th[adj_th < filter] <- 0\n\t\t    cat_non_zero <- rownames(adj_th)[rowSums(adj_th)!=0]\n\t\t    adj_th <- adj_th[cat_non_zero, cat_non_zero]\n\t\t    #print(adj_th)\n\t\t    #g <- graph.adjacency(adjmatrix = adj_th, mode = \"min\", weighted = TRUE, diag = FALSE) #note minimum because the min probability\n\t\t    g <- graph.adjacency(adjmatrix = adj_th, mode = \"directed\", weighted = TRUE, diag = FALSE) #note mi\n\t\t    degree_rs <- mean(degree(g))\n\t\t  \t#print(paste(\"Degree_RS\", degree_rs))\n\t\t    filter <- filter + increment\n\t\t    \n\t\t    kk <- 0\n\t\t    kkk<-1\n\t\t  }\n\t\t  #print(mean(degree(g)))\n\t\t  #print(\"Degreeeeee\")\n\t\t  #print(degree(g))\n\t\t  #print(max(degree(g),na.rm=TRUE))\n\t\t  \n\t\t  #print(V(g)$size)\n\t\t\t#fc <- fastgreedy.community(g); colors <- rainbow(max(membership(fc)))\n\t\t\t#V(g)$color = colors[membership(fc)]\n\t\t\t#V(g)$membership <- fc$membership\n\t\t\t\n\t\t\t#if(length(V(g)) < (prop_to_lab * length(rownames(nodes))) )\n\t\t  #\tV(g)$label = as.character(nodes$subd_name[match(V(g)$name, nodes$Id)]) #watch out with the name of the column!!\n\t\t  #else\n\t\t  #{\n\t\t  \t#print(\"Filtering labels...\")\n\t\t  #\tnod_max_com <- get_max_com(g)  #get the nodes with max degree per community\n\t\t  \t#print(nod_max_com)\n\t\t  #\tV(g)$label <- ''\n\t\t  \t#V(g)$label[V(g)$name %in% nod_max_com] <- as.character(nodes$subd_name[match(nod_max_com, nodes$Id)])\n\t\t  #\tnodes_to_label <- V(g)$name[V(g)$name %in% nod_max_com] #to get the order of iGraph\n\t\t  #\tV(g)$label[V(g)$name %in% nod_max_com] <- as.character(nodes$subd_name[match(nodes_to_label, nodes$Id)])\n\t\t  #}\n\t\t\t # set.seed(pl_seed)\n\t\t   # g$layout <- layout.fruchterman.reingold(g)\n\t\t \n\t\t    if(pl==TRUE) #IN SOME cases just want the graph but not the plot\n\t\t    {\n\t\t    \ttitle_th <- paste(\"Only threshold\" , title)\n\t\t    \tpar(mfrow=c(1,1))\n\t\t    \tplot_graph_smart(g, main = title_th, sub_add = paste(subtitle, \"Seed plot: \",\n\t\t    \t\t\t                pl_seed, '| Threshold for links: ', filter-increment),lay = \"fr\", cex = cex)\n\t\t    }\n\t\t   g_th <- g\n\t\t   #flagging edges from threshold graph to future combination\n\t\t   E(g_th)$threshold <- 1\n  }#end mean degree elements\n  \n  #g_mst_copy <<- g_mst\n  #g_th_copy <<- g_th\n  \n  #MERGING GRAPHS.....\n  #E(g)$weight <- E(g)$weight/max(E(g)$weight, na.rm=TRUE) #NORMALIZING BETWEEN 0 AND 1\n  #first Full + MST\n  print(\"Merging graphs\")\n  g <- graph.union(g_full, g_mst, g_th) #note union is computed by NAME, not by ID, so it is correct to our propose\n\tprint(\"Union created\")\n  E(g)$weight <- E(g)$weight_1\n  E(g)$mst[is.na(E(g)$mst)] <- 0\n\tE(g)$threshold[is.na(E(g)$threshold)] <- 0\n\tE(g)$used <- E(g)$mst + E(g)$threshold #union\n\tprint(\"Deliting edges ...\")\n\t#g <- delete.edges(g, E(g)[E(g)$used < 1]) #podding edges that are not in mst and not in Threshold graph MOVED TO EACH FUNCTION\n\t#E(g)$weight_used[E(g)$used >= 1] <- E(g)$weight[E(g)$used >= 1] #storing weights of edges that are the ones to apply the filter\n\t\n  print(\"Edges_deleted...\")\n  #E(g)$weight[!is.na(E(g)$sim)] <- E(g)$sim #adding mst weights in mst they are in sim not in weight since in weight_1 they are distances.\n  #E(g)$weight[is.na(E(g)$weight)] <- E(g)$weight_2[is.na(E(g)$weight)]\n  #E(g)$weight <- E(g)$sim #adding mst weights in mst they are in sim not in weight since in weight_1 they are distances.\n  #E(g)$weight[is.na(E(g)$weight)] <- E(g)$weight_2[is.na(E(g)$weight)]\n  \n  \n  if(pl==TRUE) #IN SOME cases just want the graph but not the plot\n  {\n  \tlab='com'\n  \tif(length(V(g)) < (prop_to_lab * length(rownames(nodes))) )\n  \t\tlab='all'\n\t\ttitle= paste(\"MST+Threshold \",title, \"\\nMean degree:\", mean_deg) \n\t\tpar(mfrow=c(1,1))\n\t\tprint(\"Ploting threshold graph\")\n\t\tg_to_plot <- delete.edges(g, E(g)[E(g)$used < 1])\n\t\tplot_graph_smart(g_to_plot, main = title, sub_add = paste(subtitle, \"Seed plot: \",\n\t\t\t\tpl_seed, '| Threshold for links: ', filter-increment),lay = \"fr\", v_label=lab, cex = cex)\n  \t\n\t\tpar(mfrow=c(1,2))\n\t\tplot_graph_smart(g_to_plot, main = 'Original Colors Taxonomy', sub = paste(\"Communities:\",length(unique(nodes$color))),lay = \"fr\", v_label='no', v_col='orig', cex = cex)\n\t\tplot_graph_smart(g_to_plot, main = 'Autodetected Community Color Assigned',lay = \"fr\", v_label='no', v_col='com', cex = cex)\n  }\n  \n  g_merge_copy <<- g\n  \n return(g)\n}\n\n\n##ploting graphs with smart labeling and colors\n#sub_add add text to the default sub or to the sub parameter\n#sub subtitle to plot\n#main, main title of the plot\n#g the graph to plot\n#cex a factor to scale all the things in the plot\n#pl_seed the seed to use in the plot\n#lay the layout to use, a mnemonic fr or drl or cir\nplot_graph_smart <- function(g, main='', sub=NULL, sub_add='', cex=1, pl_seed=\"69\", lay='fr', v_label='com', v_size='degree', v_col='com')\n{\n\t\n\t######GRAPH PROPERTIES\n\tset.seed(pl_seed)\n\tif(lay=='fr') #force directed\n\t\tg$layout <- layout.fruchterman.reingold(g)\n\tif(lay=='drl')\n\t\tg$layout <- layout.drl(g)\n\tif(lay=='rt') #tree oriented \n\t\tg$layout <- layout.reingold.tilford(g)\n\tif(lay=='sph') #sphere\n\t\tg$layout <- layout.sphere(g)\n\t#fc <- fastgreedy.community(g, weights = NULL); colors <- rainbow(length(fc))\n\tprint(\"Detecting communities...\")\n\t#fc <- fastgreedy.community(g); colors <- rainbow(length(fc))\n\tfc <- infomap.community(g); colors <- rainbow(length(fc))\n\tprint(\"Degree computing...\")\n\tmean_degree <- trunc(mean(degree(g)))\n\tsub_default <- paste( 'Number of nodes: ', length(V(g)), '/', nrow(nodes), '  | Mean degree: ', mean_degree, ' | Detected communities: ', length(fc))\n\t\n\tif(is.null(sub))\n\t\tsub <-\tsub_default\n\t\n\t#adding sub_add to the subtitle \n\tif(!is.null(sub_add))\n\t\tsub <- paste(sub_add, '\\n', sub)\n\tV(g)$membership <- fc$membership\n\t\n\t######## VERTICES PROPERTIES\n\tprint(\"labeling...in\")\n\tif(v_label =='com')\n\t{\n\t\t#g_mst_copy<<- g_mst\n\t\tnod_max_com <- get_max_com(g)  #get the nodes with max degree per community\n\t\t#print(nod_max_com)\n\t\tV(g)$label <- ''\n\t\tnodes_to_label <- V(g)$name[V(g)$name %in% nod_max_com] #to get the order of iGraph\n\t\tV(g)$label[V(g)$name %in% nod_max_com] <- as.character(nodes$subd_name[match(nodes_to_label, nodes$Id)])\n\t\t#V(g)$label = as.character(nodes$subd_name[match(V(g)$name, nodes$Id)])\n\t}\n\tif(v_label == 'all')\n\t\tV(g)$label = as.character(nodes$subd_name[match(V(g)$name, nodes$Id)])\n\tif(v_label == 'no')\n\t\tV(g)$label <- ''\n\t\n\tprint(\"Coloring .... in\")\n\t#color\n\tif(v_col == 'com')\n\t\tV(g)$color <- colors[membership(fc)]\n\tif(v_col == 'orig')\n\t\tV(g)$color <- as.character(nodes$color[match(V(g)$name, nodes$Id)]) \n\t\n\t#size\n\tprint(\"Sizing Nodes...\")\n\tif(is.numeric(v_size)==TRUE)\n\t\tV(g)$size <- v_size\n\tif(v_size=='degree')\n\t\tV(g)$size <- (degree(g)/max(degree(g))) * 5\n\tif(v_size == 'orig')\n\t{\n\t\tV(g)$size <- as.numeric(nodes$size[match(V(g)$name, nodes$Id)]) \n\t\tV(g)$size <- (V(g)$size/max(V(g)$size)) *5\n\t}\n\t\t\n\t\n\t#edge.label.color=\"gray\",\n\t\n\tprint(\"Plotting Graph....in \")\n\tplot.igraph(g, \n\t\tvertex.label.cex=0.7*cex, \n\t\tmain= list(main, cex=1*cex),\n\t\tvertex.label.font=0, \n\t\tvertex.label.family='Helvetica', \n\t\tvertex.label.color='black', \n\t\tedge.label.cex=0.6*cex,\n\t\tedge.label.family=\"Helvetica\",\n\t\tsub=list(sub,\tcex=0.8*cex),\n\t\tasp=FALSE\n\t)\n}\n\n\n#get a vector of nodes with max degree value inside each community\n# graph with membership\n# n_com_max number of maximum values per category, in case they are equal max values\nget_max_com <- function(g, n_com_max=1, n_com)\n{\n\tV(g)$degree <- degree(g)\n\t#getting betweenness centrality\n\t#g_lcc <- g\n\t\n\t#\tdecompose.graph(g)\n\t#g_lcc <- g_lcc[[1]] #largest connected component\n\t#print(E(g_lcc)$weight==0)\n\t#V(g_lcc)$betweenness <- betweenness(g_lcc, directed = FALSE)\n\t#E(g_lcc)[E(g_lcc)$weight<=0] <- NULL\n\t#E(g_lcc)[is.na(E(g_lcc)$weight)] <- NULL\n\t#V(g)$betweenness[V(g_lcc)$name] <- V(g_lcc)$betweenness\n\t\n\tV(g)$betweenness <- betweenness(g)\n\t\n\tif(is.null(V(g)$membership))\n\t{\n\t  #fc <- fastgreedy.community(g)\n\t  fc <- infomap.community(g)\n\t  V(g)$membership <- fc$membership\n\t}\n\t  \n\tcommunities <- unique(V(g)$membership)\n\tmax_total <- vector()\n\t#print(communities)\n\tfor(com in communities)\n\t{\n\t\t#print(paste(\"comm\", com))\n\t\tmax_deg <- sort(V(g)$degree[V(g)$membership==com ], decreasing=TRUE)\n\t\tmax_betw <- sort(V(g)$betweenness[V(g)$membership==com], decreasing = TRUE)\n\t\t\n\t\tn_max <- as.integer(log(length(max_deg), base=2))  #maximum nodes to label as a function of log base 2 \n\t\t\t\n\t\tmax_com <- V(g)$name[V(g)$membership==com & V(g)$degree > max_deg[n_max]]\t\n\t\tmax_com <- unique(c(max_com, V(g)$name[V(g)$membership==com & V(g)$betweenness > max_betw[n_max]]))\n\t\t\n\t\t#if(length(max_com) > n_com_max) #avoiding too much max in the same community\n\t\t\t#max_com <- max_com[1:n_com_max]\n\t\t\n\t\tif(length(max_com)==0 && length(max_deg) > 1) #in case no one node has been labaled in the community\n\t\t{\n\t\t\tnodes_com <- V(g)$name[V(g)$membership==com]\t\n\t\t\tmax_com <- nodes_com[trunc(length(nodes_com)/2)]\n\t\t}\n\t\t\n\t\tmax_total <- c(max_total, max_com)\n\t}\n\t\n\treturn(max_total)\n}\n\n#plots an animation of rs varying its mean degree\n#if name of a file not defined file_mat, then adj must be defined. \n# file_mat is the name of the file csv that contains a matrix in path_rs \n# name without extension and point\n# file_name used in case file_mat not provided, a name for the resulting GiF\n# prop_to_lab threshold of number of nodes to show all labels of the nodes\nplot_rs_gif <- function( file_mat=NULL, mean_degree=c(4),  mst=FALSE, pl_seeds=1, name_file='UnNamed', width=1600, height=1600, prop_to_lab=0.2)\n{\n\tlibrary(animation)\n\t\n\t\n\tprev_wd <- getwd()\n\tsetwd(file.path(path_rs,\"GIFs\"))\n\t\n\tfor(seed in pl_seeds)\n\t{\n\t\tfile_name <- paste(file_mat,'_sd', seed,  '.gif', sep='')\n\t\tif(file.exists(file.path(path_rs, file_name) ) )\n\t\t\tfile.remove(file_name) #if not removed it will not be updated\n\t\t#file_movie <- file.path(path_rs,file_name)\n\t\tsaveGIF({\n\t\t\tcount =0\n\t\t\tfor(i in mean_degree){\n\t\t\t\tif(count==0 & mst==TRUE)\n\t\t\t\t\tplot_rs(path_rs = file.path(path_rs,paste(file_mat,\".RData\",sep='')), mean_degree = i, mst=TRUE, pl_seed=seed, prop_to_lab=prop_to_lab)\n\t\t\t\telse\n\t\t\t\t\tplot_rs(path_rs = file.path(path_rs,paste(file_mat,\".RData\",sep='')), mean_degree = i, mst=FALSE, pl_seed=seed, prop_to_lab=prop_to_lab)\n\t\t\t\tcount <- count + 1\n\t\t\t}\n\t\t\t\n\t\t}, interval = 3, movie.name = file_name, ani.width = width, ani.height = height)\n\t}\n\tsetwd(prev_wd)\n\tprint(\"Done!\")\n}\t\n\n\nread_mat_rs <- function(file_mat)\n{\n\tadj_df <- read.csv(file.path(path_rs, paste(file_mat,\".csv\",sep='')), check.names=FALSE)\n\trow.names(adj_df) <- adj_df[,1]; adj_df[,1]<-NULL\n\tadj <- as.matrix(adj_df)\n\treturn(adj)\n}\n\n#create in batch a bunge of research spaces\n#batch_create_rs(n=-1, init_years = c(1987, 1987, 1990, 2000, 2010), end_years = c(2014, 1989, 1999, 2009, 2014), min_awjf = c(0.25,0.50,0.75,1,1.25,1.5,1.75,2,2.5))\n#useful to generate RS for the Overlapping Tree Method OTN\n#batch_create_rs(n=-1, init_years = c(1971, 1971, 1980, 1990, 2000, 2010), end_years = c(2014, 1980, 1989, 1999, 2009, 2014), min_awjf = c(0.25,0.50,0.75,1,1.25,1.5,1.75,2,2.5), dir_otn = '')\n\nbatch_create_rs <- function(n=-1,init_years, end_years, min_awjf, min_prod, min_awgh, min_jfrac, sim='pr', mst=TRUE, dir_otn='OTN')\n{\n\tfor(i in 1:length(init_years) )\n\t{\n\t\tfor(j in 1:length(min_awjf))\n\t\t\tcreate_rs(n=n, init = init_years[i], end = end_years[i], min_awjf = min_awjf[j], pl = TRUE, sim = sim, mst = mst, dir_otn=dir_otn)\n\t\t#for(j in 1:length(min_prod))\n\t\t#\tcreate_rs(n=n,init = init_years[i], end = end_years[i], min_prod = min_prod[j], pl = TRUE)\n\t\t#for(j in 1:length(min_awgh))\n\t\t#\t\tcreate_rs(n=n, init = init_years[i], end = end_years[i], min_awgh = min_awgh[j], pl = TRUE)\n\t\t#for(j in 1:length(min_jfrac))\n\t\t\t#create_rs(n=n,init = init_years[i], end = end_years[i], min_jfrac = min_jfrac[j], pl = TRUE)\n\t}\n}\n\n\nbatch_create_gif <- function(mean_degree=c(4,5,6,11,17, 28, 35), mst=TRUE)\n{\n\tfiles <- list.files(path = path_rs)\n\tfor(file in files)\n\t{\n\t\tif(grepl(\".RData\", file) )\n\t\t{\n\t\t\tfile_rdata <- substr(file, start = 1, stop = nchar(file)-6)\n\t\t\tplot_rs_gif(file_mat = file_rdata, mean_degree = mean_degree, mst = mst )\n\t\t}\n\t\t\t\n\t}\n}\n\n\n#batch analyzis of research spaces stored in file rs_path, to compare them against the \n#benchmark map, or the original map of the classification\n#getting data to overlay\n#n_eval number of rs in the top\n#config(db='gscholar', cl='ucsd', pr='cnt')\n#data_to_overlay <- get_data_to_overlay(n=-1, init=2001, end=2010, by = 5, cum=FALSE, min_prod = 500, what_agg=\"wgh_jfrac\", agg_fun = sum, list_prod = NaN)\nbatch_evaluation_rs <- function(data_to_overlay, mean_degrees=c(9), n_eval=7, pl_roc=FALSE)\n{\n\tmaps_top <- data.frame()  #to add the resulting top maps per mean_degree\n\tfiles <- list.files(path = path_rs) #folder with research spaces in evaluation\n\t\n\t#storing benchmark map\n\tdata_rocs <- overlay_data_3(data_to_use = data_to_overlay, what_eval = 'rca', min_to_grw = 0.5, min_to_dev = 1, pl_base=FALSE, pl = FALSE, pl_roc = FALSE,pl_base_mst = FALSE)\n\trocs_agg_bench <- plot_rocs(data_eval_roc = data_rocs, pl=FALSE)\n\t\n\tfor(mean_degree in mean_degrees)\n\t{\n\t\t\t\t#adding first the benchmark map for this iteration\n\t\t\t\tdata_auc_total <- data.frame() #dataframe to store the complete information of evaluation\n\t\t\t\tmap <- taxo\n\t\t\t\trocs_agg <-data.frame(map, mean_degree, rocs_agg_bench)\n\t\t\t\t\n\t\t\t\tdata_auc_total <- rbind(data_auc_total, rocs_agg) \n\t\t\t\t\n\t\t\t\t# agregating to total dataframe\n\t\t\t\tprint(\"Computing and aggregating ROC results to a data frame. Analyzing all RS\")\n\t\t\t\tfor(file in files)\n\t\t\t\t{\n\t\t\t\t\tif(grepl(\".RData\", file) )\n\t\t\t\t\t{\n\t\t\t\t\t\trs_name <- substr(file, start = 1, stop = nchar(file)-6)\n\t\t\t\t\t\tdata_rocs <- overlay_data_3(data_to_use = data_to_overlay, what_eval = 'rca', min_to_grw = 0.5, min_to_dev = 1, pl_base=FALSE, pl = FALSE, pl_roc = pl_roc, rs_to_eval = rs_name, rs_mean_degree=mean_degree, pl_base_mst = FALSE)\n\t\t\t\t\t\trocs_agg <- plot_rocs(data_eval_roc = data_rocs, pl=FALSE)\n\t\t\t\t\t\tmap <- rs_name\n\t\t\t\t\t\trocs_agg <-data.frame(map, mean_degree, rocs_agg)\n\t\t\t\t\t\t\n\t\t\t\t\t\tdata_auc_total <- rbind(data_auc_total, rocs_agg) # agregating to total dataframe\n\t\t\t\t\t\t\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t}\n\t\t\t\n\t\t\t\tproducers <- data_auc_total$prod[data_auc_total$map == taxo]; \n\t\t\t\tprev_int <- data_auc_total$prev_interval[1]\n\t\t\t\tinterval <- data_auc_total$inter[[1]]\n\t\t\t\t#choosing only the most useful maps per ranking \n\t\t\t\tmaps_o <- unique(data_auc_total$map)\n\t\t\t\t\n\t\t\t\t#ranking per mean of auc_I_A\n\t\t\t\tmeans_maps <- aggregate(auc_I_A~map, data_auc_total, FUN=mean)\n\t\t\t\tmeans_maps_auc_g_d <- aggregate(auc_G_D~map, data_auc_total, FUN=mean)\n\t\t\t\tmeans_maps_auc_u_d <- aggregate(auc_U_D~map, data_auc_total, FUN=mean)\n\t\t\t\tmeans_maps <- merge(means_maps, means_maps_auc_g_d, by='map')\n\t\t\t\tmeans_maps <- merge(means_maps, means_maps_auc_u_d, by='map')\n\t\t\t\t\n\t\t\t\tmaps_sort <- means_maps[order(means_maps$auc_U_D, decreasing = TRUE),]\n\t\t\t\t\n\t\t\t\t#sorting per three criterias in herarchy order\n\t\t\t\tmaps_sort <- maps_sort[order( -trunc(maps_sort$auc_U_D*100),-trunc(maps_sort$auc_G_D*100), -trunc(maps_sort$auc_I_A*100) ),]\n\t\t\t\t\n\t\t\t\t#excluding per most of NAs in evaluation (few areas)\n\t\t\t\tmaps_excluded <- vector() #excluding per number of NAs in their auc_I_A\n\t\t\t\tfor(map in maps_o)\n\t\t\t\t{\n\t\t\t\t  \tif(sum(is.na(data_auc_total[data_auc_total$map == map, 'auc_I_A'])) > length(producers)*0.35 ) \n\t\t\t\t\t  \tmaps_excluded <- c( maps_excluded, map) \t\t\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\t#defining maps ranked to eval\n\t\t\t\tmaps_eval <- maps_sort[!(maps_sort$map %in% maps_excluded),]\n\t\t\t\tnmaps <- nrow(maps_eval)\n\t\t\t\t\n\t\t\t\t#PLOTING \n\t\t\t\tplot_maps_comparation(data_auc_total = data_auc_total , maps_eval=maps_eval, n_eval=n_eval) #ploting only the top n_eval\n\t\t\t\t\n\t\t\t\trank <- c(1:nmaps)\n\t\t\t\t\n\t\t\t\tmaps_top_mean_deg <- data.frame(prev_int, interval, rank, mean_degree, maps_eval)\n\t\t\t\trownames(maps_top_mean_deg) <- rank\n\t\t\t\t\n\t\t\t\t#adding to top plots \n\t\t\t\tmaps_top <- rbind(maps_top, maps_top_mean_deg)\n\t\t\t\t\n\t}#end mean_degree LOOP\n\t\n\treturn(maps_top)\t\n}\n\n\nplot_maps_comparation <- function(data_auc_total, maps_eval, n_eval=1, mean_degree=4)\n{\n\tcolors <- rainbow(n_eval)\n\tlinetype <- c(1,1, c(1:(n_eval-2)))\n\tplotchar <- c(1:n_eval)\n\tproducers_ax <- unique(data_auc_total$prod)\n\tprev_int <- data_auc_total$prev_interval[1]\n\tinterval <- data_auc_total$inter[[1]]\n\t#starting evaluation plots\n\tpar(mfrow=c(2,2)) #starts template for roc plots \n\tfor(auc_eval in c('auc_I_A', 'auc_G_D', 'auc_U_D'))\n\t{\n\t\tplot(data_auc_total[data_auc_total$map==taxo, auc_eval], xaxt='n', ylab=auc_eval, xlab=\"Producer\", col=\"red\",  ylim = c(0,1), type=\"n\")\n\t\taxis(1, labels=producers_ax, at =(1:length(producers_ax)) )\n\t\ti <- 1\n\t\tfor(map in maps_eval$map[1:n_eval])#considering the same interval\n\t\t{\n\t\t\tlines(producers_ax, data_auc_total[data_auc_total$map == map, auc_eval], type=\"b\", col=colors[i], lty=linetype[i], pch=plotchar[i])\n\t\t\ti <- i+1\n\t\t}\n\t}\t\n\t#legend in another plot\n\tplot.new()\n\tif(i<4)  maps_eval$map <- strtrim(maps_eval$map, 15)\n\tlegend <- paste(maps_eval[1:n_eval,'map'], '|', \n\t\tround(maps_eval[1:n_eval,'auc_I_A'],3), '|',\n\t\tround(maps_eval[1:n_eval,'auc_G_D'],3), '|',\n\t\tround(maps_eval[1:n_eval,'auc_U_D'],3))\n\tlegend(\"topleft\", legend=legend,  col=colors, pch=plotchar, lty=linetype, title=\"Map | avg(AUC_I_A) | avg(AUC_G_D) | avg(AUC_U_D)\", cex=0.5, pt.cex = 0.8)\n\t\n\t\n\t\n}\n\n\n#boxplots\nplot_maps_comparation_box <- function(data_auc_total, maps_eval, n_eval=1, mean_degree=4)\n{\n  colors <- rainbow(n_eval)\n  #linetype <- c(1,1, c(1:(n_eval-2)))\n  #plotchar <- c(1:n_eval)\n  producers_ax <- unique(data_auc_total$prod)\n  prev_int <- data_auc_total$prev_interval[1]\n  interval <- data_auc_total$inter[[1]]\n  maps_eval$map <- strtrim(maps_eval$map, 4)\n  data_auc_total$map <- strtrim(data_auc_total$map, 4)\n  #starting evaluation plots\n  par(mfrow=c(1,3)) #starts template for roc plots \n  boxplot(auc_I_A~map, data=data_auc_total, ylab='AUC Inactive to Active')\n  aov_I_A <- aov(auc_I_A~map, data=data_auc_total)\n  print(\"ANOVA Analysis for Inactive to Active transition\")\n  print(summary(aov_I_A))\n  \n  boxplot(auc_G_D~map, data=data_auc_total, ylab='AUC Growing to Developed')\n  aov_G_D <- aov(auc_G_D~map, data=data_auc_total)\n  print(\"ANOVA Analysis for Growing to Developed transition\")\n  print(summary(aov_G_D))\n  \n  boxplot(auc_U_D~map, data=data_auc_total, ylab='AUC Undeveloped to Developed')\n  \n  aov_U_D <- aov(auc_U_D~map, data=data_auc_total)\n  print(\"ANOVA Analysis for Undeveloped to Developed transition\")\n  print(summary(aov_U_D))\n  print(\"Maps' means\")\n  print(maps_eval)\n  #plot.new()\n  #legend <- paste(maps_eval[1:n_eval,'map'], '|', \n                  #round(maps_eval[1:n_eval,'auc_I_A'],3), '|',\n                  #round(maps_eval[1:n_eval,'auc_G_D'],3), '|',\n                  #round(maps_eval[1:n_eval,'auc_U_D'],3))\n  \n  #legend(\"topleft\", legend=legend,   title=\"Map | avg(AUC_I_A) | avg(AUC_G_D) | avg(AUC_U_D)\")\n  #print(legend)\n  \n}\n\n#the graph g must include weight and weight_bench\nplot_maps_correlation <- function(g)\n{\n\tpar(mfrow=c(1,1))\n\tx <- E(gms)$weight/max(E(gms)$weight, na.rm = TRUE)\n\tE(gms)$weight_t <- E(gms)$weight/max(E(gms)$weight, na.rm = TRUE)\n\t\n\ty <- E(gms)$weight_bench/max(E(gms)$weight_bench, na.rm = TRUE)\n\tE(gms)$weight_bench_t <- E(gms)$weight_bench/max(E(gms)$weight_bench, na.rm = TRUE)\n\tx_from <- E(gms)[from(E(gms))]\n\t\n\ttemp_mat <- data.frame(x,y)\n\t\n\tx_temp <- E(gms)$weight_t[E(gms)$weigth_t>0.8]\n\ty_temp <- \n\tplot(x, y,\n\t\tmain = paste(\"Correlation between links Research-Space and \" , taxo),\n\t\txlab = \"Research Space normalized link weights\",\n\t\tylab = paste(taxo,\"normalized link weights.\"),\n\t\tcol = \"Blue\",\n\t\tasp = FALSE,\n\t\txlim = c(0,1),\n\t\tylim = c(0,1)\n\t\t\n\t\t)\n\t\n\tlines(c(0,1), c(0,1), col=\"lightgray\", lty=3)\n\tabline(lm(y~x), col=\"grey\", lwd=3) # regression line (y~x)\n\t\n\tcorr <- cor.test(y,x)\n\t#x_quar<-1; y_quar<-4\n\t#if(corr$estimate<0){x_quar<-1;y_quar<-2} \n\t#x_cor <- quantile(x,na.rm=TRUE)[x_quar]\n\t#y_cor <- quantile(y,na.rm=TRUE)[y_quar]\n\ttext(0.8,0.1, paste(\"R=\",round(corr$estimate,2),sep=\"\"),font=16, pos=4)\n\t\n}\n",
    "created" : 1447850361125.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3334046438",
    "id" : "EC9E3229",
    "lastKnownWriteTime" : 1455196350,
    "path" : "~/Dropbox/doctorado/MIT_PROJECT/TESIS_RESEARCH_SPACE/CODES/RS2015/createResearchSpace.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 0,
    "source_on_save" : false,
    "type" : "r_source"
}